diff --git a/Cargo.lock b/Cargo.lock
index 84d7681a165..f7561330fe6 100644
--- a/Cargo.lock
+++ b/Cargo.lock
@@ -2274,6 +2274,12 @@ version = "1.0.7"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "3f9eec918d3f24069decb9af1554cad7c880e2da24a9afd88aca000531ab82c1"
 
+[[package]]
+name = "foldhash"
+version = "0.1.5"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "d9c4f5dac5e15c24eb999c26181a6ca40b39fe946cbe4c263c7209467bc83af2"
+
 [[package]]
 name = "foreign-types"
 version = "0.3.2"
@@ -2712,6 +2718,7 @@ dependencies = [
  "keyring",
  "lazy_static",
  "lopdf",
+ "lru",
  "mcp-core",
  "mcp-server",
  "oauth2",
@@ -2873,6 +2880,11 @@ name = "hashbrown"
 version = "0.15.2"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "bf151400ff0baff5465007dd2f3e717f3fe502074ca563069ce3a6629d07b289"
+dependencies = [
+ "allocator-api2",
+ "equivalent",
+ "foldhash",
+]
 
 [[package]]
 name = "hashlink"
@@ -3862,6 +3874,15 @@ dependencies = [
  "weezl",
 ]
 
+[[package]]
+name = "lru"
+version = "0.12.5"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "234cf4f4a04dc1f57e24b96cc0cd600cf2af460d4161ac5ecdd0af8e1f3b2a38"
+dependencies = [
+ "hashbrown 0.15.2",
+]
+
 [[package]]
 name = "malloc_buf"
 version = "0.0.6"
diff --git a/crates/goose-mcp/Cargo.toml b/crates/goose-mcp/Cargo.toml
index 352c42e89a5..31c299c790d 100644
--- a/crates/goose-mcp/Cargo.toml
+++ b/crates/goose-mcp/Cargo.toml
@@ -60,6 +60,7 @@ hyper = "1"
 serde_with = "3"
 which = "6.0"
 glob = "0.3"
+lru = "0.12"
 
 
 [dev-dependencies]
diff --git a/crates/goose-server/src/commands/agent.rs b/crates/goose-server/src/commands/agent.rs
index b9ab64f41c3..a9d815cfdde 100644
--- a/crates/goose-server/src/commands/agent.rs
+++ b/crates/goose-server/src/commands/agent.rs
@@ -1,10 +1,7 @@
-use std::sync::Arc;
-
 use crate::configuration;
 use crate::state;
 use anyhow::Result;
 use etcetera::{choose_app_strategy, AppStrategy};
-use goose::agents::Agent;
 use goose::config::APP_STRATEGY;
 use goose::scheduler_factory::SchedulerFactory;
 use tower_http::cors::{Any, CorsLayer};
@@ -30,10 +27,7 @@ pub async fn run() -> Result<()> {
     let secret_key =
         std::env::var("GOOSE_SERVER__SECRET_KEY").unwrap_or_else(|_| "test".to_string());
 
-    let new_agent = Agent::new();
-    let agent_ref = Arc::new(new_agent);
-
-    let app_state = state::AppState::new(agent_ref.clone(), secret_key.clone());
+    let app_state = state::AppState::new(secret_key.clone()).await;
 
     let schedule_file_path = choose_app_strategy(APP_STRATEGY.clone())?
         .data_dir()
@@ -42,8 +36,8 @@ pub async fn run() -> Result<()> {
     let scheduler_instance = SchedulerFactory::create(schedule_file_path).await?;
     app_state.set_scheduler(scheduler_instance.clone()).await;
 
-    // NEW: Provide scheduler access to the agent
-    agent_ref.set_scheduler(scheduler_instance).await;
+    // TODO: Once we have per-session agents, each agent will need scheduler access
+    // For now, we'll handle this when agents are created in AgentManager
 
     let cors = CorsLayer::new()
         .allow_origin(Any)
diff --git a/crates/goose-server/src/routes/agent.rs b/crates/goose-server/src/routes/agent.rs
index 2a591182622..767ea013000 100644
--- a/crates/goose-server/src/routes/agent.rs
+++ b/crates/goose-server/src/routes/agent.rs
@@ -102,6 +102,15 @@ pub struct ErrorResponse {
     error: String,
 }
 
+#[derive(Serialize, utoipa::ToSchema)]
+pub struct AgentStatsResponse {
+    agents_created: usize,
+    agents_cleaned: usize,
+    cache_hits: usize,
+    cache_misses: usize,
+    active_agents: usize,
+}
+
 #[utoipa::path(
     post,
     path = "/agent/start",
@@ -120,7 +129,7 @@ async fn start_agent(
 ) -> Result<Json<StartAgentResponse>, StatusCode> {
     verify_secret_key(&headers, &state)?;
 
-    state.reset().await;
+    // No longer reset the global agent - each session gets its own
 
     let session_id = session::generate_session_id();
     let counter = state.session_counter.fetch_add(1, Ordering::SeqCst) + 1;
@@ -214,7 +223,11 @@ async fn add_sub_recipes(
 ) -> Result<Json<AddSubRecipesResponse>, StatusCode> {
     verify_secret_key(&headers, &state)?;
 
-    let agent = state.get_agent().await;
+    let session_id = session::Identifier::Name(payload.session_id.clone());
+    let agent = state.get_agent(session_id).await.map_err(|e| {
+        tracing::error!("Failed to get agent for session: {}", e);
+        StatusCode::INTERNAL_SERVER_ERROR
+    })?;
     agent.add_sub_recipes(payload.sub_recipes.clone()).await;
     Ok(Json(AddSubRecipesResponse { success: true }))
 }
@@ -236,7 +249,11 @@ async fn extend_prompt(
 ) -> Result<Json<ExtendPromptResponse>, StatusCode> {
     verify_secret_key(&headers, &state)?;
 
-    let agent = state.get_agent().await;
+    let session_id = session::Identifier::Name(payload.session_id.clone());
+    let agent = state.get_agent(session_id).await.map_err(|e| {
+        tracing::error!("Failed to get agent for session: {}", e);
+        StatusCode::INTERNAL_SERVER_ERROR
+    })?;
     agent.extend_system_prompt(payload.extension.clone()).await;
     Ok(Json(ExtendPromptResponse { success: true }))
 }
@@ -264,7 +281,11 @@ async fn get_tools(
 
     let config = Config::global();
     let goose_mode = config.get_param("GOOSE_MODE").unwrap_or("auto".to_string());
-    let agent = state.get_agent().await;
+    let session_id = session::Identifier::Name(query.session_id.clone());
+    let agent = state.get_agent(session_id).await.map_err(|e| {
+        tracing::error!("Failed to get agent for session: {}", e);
+        StatusCode::INTERNAL_SERVER_ERROR
+    })?;
     let permission_manager = PermissionManager::default();
 
     let mut tools: Vec<ToolInfo> = agent
@@ -319,7 +340,11 @@ async fn update_agent_provider(
 ) -> Result<StatusCode, impl IntoResponse> {
     verify_secret_key(&headers, &state).map_err(|e| (e, String::new()))?;
 
-    let agent = state.get_agent().await;
+    let session_id = session::Identifier::Name(payload.session_id.clone());
+    let agent = state.get_agent(session_id).await.map_err(|e| {
+        tracing::error!("Failed to get agent for session: {}", e);
+        (StatusCode::INTERNAL_SERVER_ERROR, String::new())
+    })?;
     let config = Config::global();
     let model = match payload
         .model
@@ -365,7 +390,7 @@ async fn update_agent_provider(
 async fn update_router_tool_selector(
     State(state): State<Arc<AppState>>,
     headers: HeaderMap,
-    Json(_payload): Json<UpdateRouterToolSelectorRequest>,
+    Json(payload): Json<UpdateRouterToolSelectorRequest>,
 ) -> Result<Json<String>, Json<ErrorResponse>> {
     verify_secret_key(&headers, &state).map_err(|_| {
         Json(ErrorResponse {
@@ -373,7 +398,13 @@ async fn update_router_tool_selector(
         })
     })?;
 
-    let agent = state.get_agent().await;
+    let session_id = session::Identifier::Name(payload.session_id.clone());
+    let agent = state.get_agent(session_id).await.map_err(|e| {
+        tracing::error!("Failed to get agent for session: {}", e);
+        Json(ErrorResponse {
+            error: format!("Failed to get agent: {}", e),
+        })
+    })?;
     agent
         .update_router_tool_selector(None, Some(true))
         .await
@@ -411,7 +442,13 @@ async fn update_session_config(
         })
     })?;
 
-    let agent = state.get_agent().await;
+    let session_id = session::Identifier::Name(payload.session_id.clone());
+    let agent = state.get_agent(session_id).await.map_err(|e| {
+        tracing::error!("Failed to get agent for session: {}", e);
+        Json(ErrorResponse {
+            error: format!("Failed to get agent: {}", e),
+        })
+    })?;
     if let Some(response) = payload.response {
         agent.add_final_output_tool(response).await;
 
@@ -424,6 +461,55 @@ async fn update_session_config(
     }
 }
 
+#[utoipa::path(
+    get,
+    path = "/agent/stats",
+    responses(
+        (status = 200, description = "Agent statistics retrieved successfully", body = AgentStatsResponse),
+        (status = 401, description = "Unauthorized - invalid secret key"),
+        (status = 500, description = "Internal server error")
+    )
+)]
+async fn get_agent_stats(
+    State(state): State<Arc<AppState>>,
+    headers: HeaderMap,
+) -> Result<Json<AgentStatsResponse>, StatusCode> {
+    verify_secret_key(&headers, &state)?;
+
+    let metrics = state.get_agent_metrics().await;
+
+    Ok(Json(AgentStatsResponse {
+        agents_created: metrics.agents_created,
+        agents_cleaned: metrics.agents_cleaned,
+        cache_hits: metrics.cache_hits,
+        cache_misses: metrics.cache_misses,
+        active_agents: metrics.active_agents,
+    }))
+}
+
+#[utoipa::path(
+    post,
+    path = "/agent/cleanup",
+    responses(
+        (status = 200, description = "Agent cleanup completed successfully", body = String),
+        (status = 401, description = "Unauthorized - invalid secret key"),
+        (status = 500, description = "Internal server error")
+    )
+)]
+async fn cleanup_agents(
+    State(state): State<Arc<AppState>>,
+    headers: HeaderMap,
+) -> Result<Json<String>, StatusCode> {
+    verify_secret_key(&headers, &state)?;
+
+    let cleaned = state.cleanup_idle_agents().await.map_err(|e| {
+        tracing::error!("Failed to cleanup agents: {}", e);
+        StatusCode::INTERNAL_SERVER_ERROR
+    })?;
+
+    Ok(Json(format!("Cleaned up {} idle agents", cleaned)))
+}
+
 pub fn routes(state: Arc<AppState>) -> Router {
     Router::new()
         .route("/agent/start", post(start_agent))
@@ -437,5 +523,7 @@ pub fn routes(state: Arc<AppState>) -> Router {
         )
         .route("/agent/session_config", post(update_session_config))
         .route("/agent/add_sub_recipes", post(add_sub_recipes))
+        .route("/agent/stats", get(get_agent_stats))
+        .route("/agent/cleanup", post(cleanup_agents))
         .with_state(state)
 }
diff --git a/crates/goose-server/src/routes/audio.rs b/crates/goose-server/src/routes/audio.rs
index 3ab4d8b83dd..814102f802d 100644
--- a/crates/goose-server/src/routes/audio.rs
+++ b/crates/goose-server/src/routes/audio.rs
@@ -410,10 +410,7 @@ mod tests {
 
     #[tokio::test]
     async fn test_transcribe_endpoint_requires_auth() {
-        let state = AppState::new(
-            Arc::new(goose::agents::Agent::new()),
-            "test-secret".to_string(),
-        );
+        let state = AppState::new("test-secret".to_string()).await;
         let app = routes(state);
 
         // Test without auth header
@@ -436,10 +433,7 @@ mod tests {
 
     #[tokio::test]
     async fn test_transcribe_endpoint_validates_size() {
-        let state = AppState::new(
-            Arc::new(goose::agents::Agent::new()),
-            "test-secret".to_string(),
-        );
+        let state = AppState::new("test-secret".to_string()).await;
         let app = routes(state);
 
         // Create a large base64 string (simulating > 25MB audio)
@@ -465,10 +459,7 @@ mod tests {
 
     #[tokio::test]
     async fn test_transcribe_endpoint_validates_mime_type() {
-        let state = AppState::new(
-            Arc::new(goose::agents::Agent::new()),
-            "test-secret".to_string(),
-        );
+        let state = AppState::new("test-secret".to_string()).await;
         let app = routes(state);
 
         let request = Request::builder()
@@ -494,10 +485,7 @@ mod tests {
 
     #[tokio::test]
     async fn test_transcribe_endpoint_handles_invalid_base64() {
-        let state = AppState::new(
-            Arc::new(goose::agents::Agent::new()),
-            "test-secret".to_string(),
-        );
+        let state = AppState::new("test-secret".to_string()).await;
         let app = routes(state);
 
         let request = Request::builder()
diff --git a/crates/goose-server/src/routes/config_management.rs b/crates/goose-server/src/routes/config_management.rs
index 5c5b8bbb2f4..ac4e4160e89 100644
--- a/crates/goose-server/src/routes/config_management.rs
+++ b/crates/goose-server/src/routes/config_management.rs
@@ -855,10 +855,7 @@ mod tests {
     use super::*;
 
     async fn create_test_state() -> Arc<AppState> {
-        let test_state = AppState::new(
-            Arc::new(goose::agents::Agent::default()),
-            "test".to_string(),
-        );
+        let test_state = AppState::new("test".to_string()).await;
         let sched_storage_path = choose_app_strategy(APP_STRATEGY.clone())
             .unwrap()
             .data_dir()
diff --git a/crates/goose-server/src/routes/context.rs b/crates/goose-server/src/routes/context.rs
index 7f23b8777fd..d3f7615e7b5 100644
--- a/crates/goose-server/src/routes/context.rs
+++ b/crates/goose-server/src/routes/context.rs
@@ -19,6 +19,8 @@ pub struct ContextManageRequest {
     pub messages: Vec<Message>,
     /// Operation to perform: "truncation" or "summarize"
     pub manage_action: String,
+    /// Session ID for the context management
+    pub session_id: String,
 }
 
 /// Response from context management operations
@@ -53,7 +55,12 @@ async fn manage_context(
 ) -> Result<Json<ContextManageResponse>, StatusCode> {
     verify_secret_key(&headers, &state)?;
 
-    let agent = state.get_agent().await;
+    use goose::session;
+    let session_id = session::Identifier::Name(request.session_id.clone());
+    let agent = state.get_agent(session_id).await.map_err(|e| {
+        tracing::error!("Failed to get agent for session: {}", e);
+        StatusCode::INTERNAL_SERVER_ERROR
+    })?;
 
     let mut processed_messages = Conversation::new_unvalidated(vec![]);
     let mut token_counts: Vec<usize> = vec![];
diff --git a/crates/goose-server/src/routes/extension.rs b/crates/goose-server/src/routes/extension.rs
index 1567fb71ecf..76e6eb2b5cf 100644
--- a/crates/goose-server/src/routes/extension.rs
+++ b/crates/goose-server/src/routes/extension.rs
@@ -111,6 +111,17 @@ async fn add_extension(
         serde_json::to_string_pretty(&raw.0).unwrap()
     );
 
+    // Extract session_id from the raw request
+    let session_id = raw
+        .0
+        .get("session_id")
+        .and_then(|v| v.as_str())
+        .map(|s| s.to_string())
+        .unwrap_or_else(|| {
+            tracing::warn!("No session_id provided in extension request, using default");
+            "default".to_string()
+        });
+
     // Try to parse into our enum
     let request: ExtensionConfigRequest = match serde_json::from_value(raw.0.clone()) {
         Ok(req) => req,
@@ -271,7 +282,12 @@ async fn add_extension(
         },
     };
 
-    let agent = state.get_agent().await;
+    use goose::session;
+    let session_identifier = session::Identifier::Name(session_id);
+    let agent = state.get_agent(session_identifier).await.map_err(|e| {
+        tracing::error!("Failed to get agent for session: {}", e);
+        StatusCode::INTERNAL_SERVER_ERROR
+    })?;
     let response = agent.add_extension(extension_config).await;
 
     // Respond with the result.
@@ -297,11 +313,37 @@ async fn add_extension(
 async fn remove_extension(
     State(state): State<Arc<AppState>>,
     headers: HeaderMap,
-    Json(name): Json<String>,
+    raw: axum::extract::Json<serde_json::Value>,
 ) -> Result<Json<ExtensionResponse>, StatusCode> {
     verify_secret_key(&headers, &state)?;
 
-    let agent = state.get_agent().await;
+    // Extract name and session_id from the raw request
+    let name = raw
+        .0
+        .get("name")
+        .and_then(|v| v.as_str())
+        .map(|s| s.to_string())
+        .unwrap_or_else(|| {
+            // For backward compatibility, try to parse as just a string
+            raw.0.as_str().map(|s| s.to_string()).unwrap_or_default()
+        });
+
+    let session_id = raw
+        .0
+        .get("session_id")
+        .and_then(|v| v.as_str())
+        .map(|s| s.to_string())
+        .unwrap_or_else(|| {
+            tracing::warn!("No session_id provided in remove extension request, using default");
+            "default".to_string()
+        });
+
+    use goose::session;
+    let session_identifier = session::Identifier::Name(session_id);
+    let agent = state.get_agent(session_identifier).await.map_err(|e| {
+        tracing::error!("Failed to get agent for session: {}", e);
+        StatusCode::INTERNAL_SERVER_ERROR
+    })?;
     match agent.remove_extension(&name).await {
         Ok(_) => Ok(Json(ExtensionResponse {
             error: false,
diff --git a/crates/goose-server/src/routes/recipe.rs b/crates/goose-server/src/routes/recipe.rs
index 84b114b82f7..b6b5572a32e 100644
--- a/crates/goose-server/src/routes/recipe.rs
+++ b/crates/goose-server/src/routes/recipe.rs
@@ -27,6 +27,8 @@ pub struct CreateRecipeRequest {
     activities: Option<Vec<String>>,
     #[serde(default)]
     author: Option<AuthorRequest>,
+    // Session ID for the recipe creation
+    session_id: String,
 }
 
 #[derive(Debug, Deserialize, ToSchema)]
@@ -116,7 +118,16 @@ async fn create_recipe(
         request.messages.len()
     );
 
-    let agent = state.get_agent().await;
+    use goose::session;
+    let session_id = session::Identifier::Name(request.session_id.clone());
+    let agent = state.get_agent(session_id).await.map_err(|e| {
+        tracing::error!("Failed to get agent for session: {}", e);
+        let error_response = CreateRecipeResponse {
+            recipe: None,
+            error: Some(format!("Failed to get agent: {}", e)),
+        };
+        (StatusCode::INTERNAL_SERVER_ERROR, Json(error_response))
+    })?;
 
     // Create base recipe from agent state and messages
     let recipe_result = agent
diff --git a/crates/goose-server/src/routes/reply.rs b/crates/goose-server/src/routes/reply.rs
index a11ce313450..454c8955680 100644
--- a/crates/goose-server/src/routes/reply.rs
+++ b/crates/goose-server/src/routes/reply.rs
@@ -26,6 +26,7 @@ use serde_json::json;
 use serde_json::Value;
 use std::{
     convert::Infallible,
+    path::PathBuf,
     pin::Pin,
     sync::Arc,
     task::{Context, Poll},
@@ -88,6 +89,7 @@ fn track_tool_telemetry(content: &MessageContent, all_messages: &[Message]) {
 struct ChatRequest {
     messages: Vec<Message>,
     session_id: Option<String>,
+    working_dir: Option<String>, // Optional, for backward compatibility
     recipe_name: Option<String>,
     recipe_version: Option<String>,
 }
@@ -201,41 +203,48 @@ async fn reply_handler(
 
     let messages = Conversation::new_unvalidated(request.messages);
 
-    let session_id = request.session_id.ok_or_else(|| {
-        tracing::error!("session_id is required but was not provided");
-        StatusCode::BAD_REQUEST
-    })?;
+    // Restore backward compatibility: auto-generate session_id if not provided
+    let session_id = request
+        .session_id
+        .unwrap_or_else(session::generate_session_id);
+
+    // Get working directory - use provided value, or default to current directory
+    let working_dir = request
+        .working_dir
+        .map(PathBuf::from)
+        .unwrap_or_else(|| std::env::current_dir().unwrap_or_else(|_| PathBuf::from(".")));
 
     let task_cancel = cancel_token.clone();
     let task_tx = tx.clone();
 
     drop(tokio::spawn(async move {
-        let agent = state.get_agent().await;
-
-        // Load session metadata to get the working directory and other config
-        let session_path = match session::get_path(session::Identifier::Name(session_id.clone())) {
-            Ok(path) => path,
+        let agent = match state
+            .get_agent(session::Identifier::Name(session_id.clone()))
+            .await
+        {
+            Ok(agent) => agent,
             Err(e) => {
-                tracing::error!("Failed to get session path for {}: {}", session_id, e);
+                tracing::error!("Failed to get agent for session {}: {}", session_id, e);
                 let _ = stream_event(
                     MessageEvent::Error {
-                        error: format!("Failed to get session path: {}", e),
+                        error: format!("Failed to get agent: {}", e),
                     },
                     &task_tx,
-                    &cancel_token,
+                    &task_cancel,
                 )
                 .await;
                 return;
             }
         };
 
-        let session_metadata = match session::read_metadata(&session_path) {
-            Ok(metadata) => metadata,
+        // Load or create session metadata
+        let session_path = match session::get_path(session::Identifier::Name(session_id.clone())) {
+            Ok(path) => path,
             Err(e) => {
-                tracing::error!("Failed to read session metadata for {}: {}", session_id, e);
+                tracing::error!("Failed to get session path for {}: {}", session_id, e);
                 let _ = stream_event(
                     MessageEvent::Error {
-                        error: format!("Failed to read session metadata: {}", e),
+                        error: format!("Failed to get session path: {}", e),
                     },
                     &task_tx,
                     &cancel_token,
@@ -245,6 +254,48 @@ async fn reply_handler(
             }
         };
 
+        // Try to read existing metadata, or create new if it doesn't exist
+        let session_metadata = match session::read_metadata(&session_path) {
+            Ok(metadata) => metadata,
+            Err(_) => {
+                // New session - create metadata
+                let new_metadata = session::SessionMetadata {
+                    working_dir: working_dir.clone(),
+                    description: format!("Session {}", session_id),
+                    schedule_id: None,
+                    message_count: 0,
+                    total_tokens: Some(0),
+                    input_tokens: Some(0),
+                    output_tokens: Some(0),
+                    accumulated_total_tokens: Some(0),
+                    accumulated_input_tokens: Some(0),
+                    accumulated_output_tokens: Some(0),
+                    extension_data: Default::default(),
+                    recipe: None,
+                };
+
+                // Save the new metadata
+                if let Err(e) = session::storage::save_messages_with_metadata(
+                    &session_path,
+                    &new_metadata,
+                    &Conversation::empty(),
+                ) {
+                    tracing::error!("Failed to create session metadata: {}", e);
+                    let _ = stream_event(
+                        MessageEvent::Error {
+                            error: format!("Failed to create session: {}", e),
+                        },
+                        &task_tx,
+                        &cancel_token,
+                    )
+                    .await;
+                    return;
+                }
+
+                new_metadata
+            }
+        };
+
         let session_config = SessionConfig {
             id: session::Identifier::Name(session_id.clone()),
             working_dir: session_metadata.working_dir.clone(),
@@ -471,7 +522,11 @@ pub async fn confirm_permission(
 ) -> Result<Json<Value>, StatusCode> {
     verify_secret_key(&headers, &state)?;
 
-    let agent = state.get_agent().await;
+    let session_id = session::Identifier::Name(request.session_id.clone());
+    let agent = state.get_agent(session_id).await.map_err(|e| {
+        tracing::error!("Failed to get agent for session: {}", e);
+        StatusCode::INTERNAL_SERVER_ERROR
+    })?;
     let permission = match request.action.as_str() {
         "always_allow" => Permission::AlwaysAllow,
         "allow_once" => Permission::AllowOnce,
@@ -523,7 +578,11 @@ async fn submit_tool_result(
         }
     };
 
-    let agent = state.get_agent().await;
+    let session_id = session::Identifier::Name(payload.session_id.clone());
+    let agent = state.get_agent(session_id).await.map_err(|e| {
+        tracing::error!("Failed to get agent for session: {}", e);
+        StatusCode::INTERNAL_SERVER_ERROR
+    })?;
     agent.handle_tool_result(payload.id, payload.result).await;
     Ok(Json(json!({"status": "ok"})))
 }
@@ -599,7 +658,7 @@ mod tests {
             });
             let agent = Agent::new();
             let _ = agent.update_provider(mock_provider).await;
-            let state = AppState::new(Arc::new(agent), "test-secret".to_string());
+            let state = AppState::new("test-secret".to_string()).await;
 
             let app = routes(state);
 
@@ -612,6 +671,7 @@ mod tests {
                     serde_json::to_string(&ChatRequest {
                         messages: vec![Message::user().with_text("test message")],
                         session_id: Some("test-session".to_string()),
+                        working_dir: None,
                         recipe_name: None,
                         recipe_version: None,
                     })
diff --git a/crates/goose-server/src/state.rs b/crates/goose-server/src/state.rs
index eacbfaf2514..9995365e085 100644
--- a/crates/goose-server/src/state.rs
+++ b/crates/goose-server/src/state.rs
@@ -1,5 +1,7 @@
+use goose::agents::manager::{AgentManager, AgentManagerConfig};
 use goose::agents::Agent;
 use goose::scheduler_trait::SchedulerTrait;
+use goose::session;
 use std::collections::HashMap;
 use std::path::PathBuf;
 use std::sync::atomic::AtomicUsize;
@@ -11,7 +13,7 @@ type AgentRef = Arc<Agent>;
 
 #[derive(Clone)]
 pub struct AppState {
-    agent: Arc<RwLock<AgentRef>>,
+    agent_manager: Arc<AgentManager>,
     pub secret_key: String,
     pub scheduler: Arc<RwLock<Option<Arc<dyn SchedulerTrait>>>>,
     pub recipe_file_hash_map: Arc<Mutex<HashMap<String, PathBuf>>>,
@@ -19,9 +21,10 @@ pub struct AppState {
 }
 
 impl AppState {
-    pub fn new(agent: AgentRef, secret_key: String) -> Arc<AppState> {
+    pub async fn new(secret_key: String) -> Arc<AppState> {
+        let agent_manager = Arc::new(AgentManager::new(AgentManagerConfig::default()).await);
         Arc::new(Self {
-            agent: Arc::new(RwLock::new(agent)),
+            agent_manager,
             secret_key,
             scheduler: Arc::new(RwLock::new(None)),
             recipe_file_hash_map: Arc::new(Mutex::new(HashMap::new())),
@@ -29,8 +32,14 @@ impl AppState {
         })
     }
 
-    pub async fn get_agent(&self) -> AgentRef {
-        self.agent.read().await.clone()
+    pub async fn get_agent(
+        &self,
+        session_id: session::Identifier,
+    ) -> Result<AgentRef, anyhow::Error> {
+        self.agent_manager
+            .get_agent(session_id)
+            .await
+            .map_err(|e| anyhow::anyhow!("Failed to get agent: {}", e))
     }
 
     pub async fn set_scheduler(&self, sched: Arc<dyn SchedulerTrait>) {
@@ -51,8 +60,14 @@ impl AppState {
         *map = hash_map;
     }
 
-    pub async fn reset(&self) {
-        let mut agent = self.agent.write().await;
-        *agent = Arc::new(Agent::new());
+    pub async fn cleanup_idle_agents(&self) -> Result<usize, anyhow::Error> {
+        self.agent_manager
+            .cleanup()
+            .await
+            .map_err(|e| anyhow::anyhow!("Failed to cleanup agents: {}", e))
+    }
+
+    pub async fn get_agent_metrics(&self) -> goose::agents::manager::AgentMetrics {
+        self.agent_manager.get_metrics().await
     }
 }
diff --git a/crates/goose-server/tests/multi_session_extension_test.rs b/crates/goose-server/tests/multi_session_extension_test.rs
new file mode 100644
index 00000000000..37673caa34d
--- /dev/null
+++ b/crates/goose-server/tests/multi_session_extension_test.rs
@@ -0,0 +1,275 @@
+use axum::body;
+use axum::http::StatusCode;
+use axum::Router;
+use axum::{body::Body, http::Request};
+use etcetera::AppStrategy;
+use serde_json::{json, Value};
+use std::sync::Arc;
+use tower::ServiceExt;
+
+async fn create_test_app() -> (Router, Arc<goose_server::AppState>) {
+    let state = goose_server::AppState::new("test-secret".to_string()).await;
+
+    // Set up scheduler
+    let sched_storage_path = etcetera::choose_app_strategy(goose::config::APP_STRATEGY.clone())
+        .unwrap()
+        .data_dir()
+        .join("schedules.json");
+    let sched = goose::scheduler_factory::SchedulerFactory::create_legacy(sched_storage_path)
+        .await
+        .unwrap();
+    state.set_scheduler(sched).await;
+
+    let app = goose_server::routes::configure(state.clone());
+    (app, state)
+}
+
+#[tokio::test]
+async fn test_extension_add_isolation_between_sessions() {
+    let (app, state) = create_test_app().await;
+
+    // Create two sessions
+    let session1_id = "ext_isolation_1";
+    let session2_id = "ext_isolation_2";
+
+    // Ensure agents exist
+    let _ = state
+        .get_agent(goose::session::Identifier::Name(session1_id.to_string()))
+        .await
+        .unwrap();
+    let _ = state
+        .get_agent(goose::session::Identifier::Name(session2_id.to_string()))
+        .await
+        .unwrap();
+
+    // Add a frontend extension to session 1 only
+    let add_ext_request = Request::builder()
+        .uri("/extensions/add")
+        .method("POST")
+        .header("content-type", "application/json")
+        .header("x-secret-key", "test-secret")
+        .body(Body::from(
+            json!({
+                "session_id": session1_id,
+                "type": "frontend",
+                "name": "test_extension",
+                "tools": [
+                    {
+                        "name": "custom_tool",
+                        "description": "A custom test tool",
+                        "input_schema": {
+                            "type": "object",
+                            "properties": {}
+                        }
+                    }
+                ],
+                "instructions": "Test extension for session 1"
+            })
+            .to_string(),
+        ))
+        .unwrap();
+
+    let add_response = app.clone().oneshot(add_ext_request).await.unwrap();
+    assert_eq!(add_response.status(), StatusCode::OK);
+
+    // Check tools for session 1 - should have the custom tool
+    let tools1_request = Request::builder()
+        .uri(&format!("/agent/tools?session_id={}", session1_id))
+        .method("GET")
+        .header("x-secret-key", "test-secret")
+        .body(Body::empty())
+        .unwrap();
+
+    let tools1_response = app.clone().oneshot(tools1_request).await.unwrap();
+    let tools1_body = body::to_bytes(tools1_response.into_body(), usize::MAX)
+        .await
+        .unwrap();
+    let tools1: Vec<Value> = serde_json::from_slice(&tools1_body).unwrap();
+
+    // Check tools for session 2 - should NOT have the custom tool
+    let tools2_request = Request::builder()
+        .uri(&format!("/agent/tools?session_id={}", session2_id))
+        .method("GET")
+        .header("x-secret-key", "test-secret")
+        .body(Body::empty())
+        .unwrap();
+
+    let tools2_response = app.oneshot(tools2_request).await.unwrap();
+    let tools2_body = body::to_bytes(tools2_response.into_body(), usize::MAX)
+        .await
+        .unwrap();
+    let tools2: Vec<Value> = serde_json::from_slice(&tools2_body).unwrap();
+
+    // Session 1 should have custom_tool
+    assert!(
+        tools1.iter().any(|t| t["name"] == "custom_tool"),
+        "Session 1 should have custom_tool"
+    );
+
+    // Session 2 should NOT have custom_tool
+    assert!(
+        !tools2.iter().any(|t| t["name"] == "custom_tool"),
+        "Session 2 should not have custom_tool"
+    );
+}
+
+#[tokio::test]
+async fn test_extension_remove_isolation() {
+    let (app, state) = create_test_app().await;
+
+    let session1_id = "ext_remove_1";
+    let session2_id = "ext_remove_2";
+
+    // Create agents
+    let agent1 = state
+        .get_agent(goose::session::Identifier::Name(session1_id.to_string()))
+        .await
+        .unwrap();
+    let agent2 = state
+        .get_agent(goose::session::Identifier::Name(session2_id.to_string()))
+        .await
+        .unwrap();
+
+    // Add same extension to both sessions directly via agent
+    let ext_config = goose::agents::ExtensionConfig::Frontend {
+        name: "shared_ext".to_string(),
+        tools: vec![rmcp::model::Tool {
+            name: "shared_tool".into(),
+            description: Some("Shared tool".into()),
+            input_schema: Arc::new(json!({"type": "object"}).as_object().unwrap().clone()),
+            output_schema: None,
+            annotations: None,
+        }],
+        instructions: Some("Shared extension".to_string()),
+        bundled: Some(false),
+        available_tools: vec![],
+    };
+
+    agent1.add_extension(ext_config.clone()).await.unwrap();
+    agent2.add_extension(ext_config).await.unwrap();
+
+    // Verify both have the extension
+    let tools1 = agent1.list_tools(None).await;
+    let tools2 = agent2.list_tools(None).await;
+
+    assert!(tools1.iter().any(|t| t.name == "shared_tool"));
+    assert!(tools2.iter().any(|t| t.name == "shared_tool"));
+
+    // Remove extension from session 1 via API
+    let remove_request = Request::builder()
+        .uri("/extensions/remove")
+        .method("POST")
+        .header("content-type", "application/json")
+        .header("x-secret-key", "test-secret")
+        .body(Body::from(
+            json!({
+                "session_id": session1_id,
+                "name": "shared_ext"
+            })
+            .to_string(),
+        ))
+        .unwrap();
+
+    let remove_response = app.oneshot(remove_request).await.unwrap();
+    assert_eq!(remove_response.status(), StatusCode::OK);
+
+    // Check tools again
+    let tools1_after = agent1.list_tools(None).await;
+    let tools2_after = agent2.list_tools(None).await;
+
+    // Session 1 should NOT have the tool anymore
+    assert!(!tools1_after.iter().any(|t| t.name == "shared_tool"));
+
+    // Session 2 should still have it
+    assert!(tools2_after.iter().any(|t| t.name == "shared_tool"));
+}
+
+#[tokio::test]
+async fn test_concurrent_extension_operations_via_api() {
+    let (app, state) = create_test_app().await;
+
+    // Create multiple sessions
+    let mut handles = vec![];
+
+    for i in 0..5 {
+        let app_clone = app.clone();
+        let state_clone = state.clone();
+
+        let handle = tokio::spawn(async move {
+            let session_id = format!("concurrent_session_{}", i);
+
+            // Ensure agent exists
+            let _ = state_clone
+                .get_agent(goose::session::Identifier::Name(session_id.clone()))
+                .await
+                .unwrap();
+
+            // Add extension via API
+            let add_request = Request::builder()
+                .uri("/extensions/add")
+                .method("POST")
+                .header("content-type", "application/json")
+                .header("x-secret-key", "test-secret")
+                .body(Body::from(
+                    json!({
+                        "session_id": &session_id,
+                        "type": "frontend",
+                        "name": format!("ext_{}", i),
+                        "tools": [
+                            {
+                                "name": format!("tool_{}", i),
+                                "description": format!("Tool for session {}", i),
+                                "input_schema": {"type": "object"}
+                            }
+                        ]
+                    })
+                    .to_string(),
+                ))
+                .unwrap();
+
+            let add_response = app_clone.clone().oneshot(add_request).await.unwrap();
+            assert_eq!(add_response.status(), StatusCode::OK);
+
+            // Verify tool was added
+            let tools_request = Request::builder()
+                .uri(&format!("/agent/tools?session_id={}", session_id))
+                .method("GET")
+                .header("x-secret-key", "test-secret")
+                .body(Body::empty())
+                .unwrap();
+
+            let tools_response = app_clone.oneshot(tools_request).await.unwrap();
+            let tools_body = body::to_bytes(tools_response.into_body(), usize::MAX)
+                .await
+                .unwrap();
+            let tools: Vec<Value> = serde_json::from_slice(&tools_body).unwrap();
+
+            // Should have the session-specific tool
+            assert!(
+                tools.iter().any(|t| t["name"] == format!("tool_{}", i)),
+                "Session {} should have tool_{}",
+                i,
+                i
+            );
+
+            // Should NOT have tools from other sessions
+            for j in 0..5 {
+                if j != i {
+                    assert!(
+                        !tools.iter().any(|t| t["name"] == format!("tool_{}", j)),
+                        "Session {} should not have tool_{}",
+                        i,
+                        j
+                    );
+                }
+            }
+        });
+
+        handles.push(handle);
+    }
+
+    // Wait for all concurrent operations to complete
+    for handle in handles {
+        handle.await.unwrap();
+    }
+}
diff --git a/crates/goose-server/tests/pricing_api_test.rs b/crates/goose-server/tests/pricing_api_test.rs
index d7b48b2a591..ad62513899b 100644
--- a/crates/goose-server/tests/pricing_api_test.rs
+++ b/crates/goose-server/tests/pricing_api_test.rs
@@ -7,8 +7,7 @@ use std::sync::Arc;
 use tower::ServiceExt;
 
 async fn create_test_app() -> Router {
-    let agent = Arc::new(goose::agents::Agent::default());
-    let state = goose_server::AppState::new(agent, "test".to_string());
+    let state = goose_server::AppState::new("test".to_string()).await;
 
     // Add scheduler setup like in the existing tests
     let sched_storage_path = etcetera::choose_app_strategy(goose::config::APP_STRATEGY.clone())
diff --git a/crates/goose-server/tests/session_api_test.rs b/crates/goose-server/tests/session_api_test.rs
new file mode 100644
index 00000000000..f9068f4b3f8
--- /dev/null
+++ b/crates/goose-server/tests/session_api_test.rs
@@ -0,0 +1,273 @@
+use axum::body;
+use axum::http::StatusCode;
+use axum::Router;
+use axum::{body::Body, http::Request};
+use etcetera::AppStrategy;
+use serde_json::{json, Value};
+use std::sync::Arc;
+use tower::ServiceExt;
+
+async fn create_test_app() -> (Router, Arc<goose_server::AppState>) {
+    let state = goose_server::AppState::new("test-secret".to_string()).await;
+
+    // Set up scheduler as required
+    let sched_storage_path = etcetera::choose_app_strategy(goose::config::APP_STRATEGY.clone())
+        .unwrap()
+        .data_dir()
+        .join("schedules.json");
+    let sched = goose::scheduler_factory::SchedulerFactory::create_legacy(sched_storage_path)
+        .await
+        .unwrap();
+    state.set_scheduler(sched).await;
+
+    let app = goose_server::routes::configure(state.clone());
+    (app, state)
+}
+
+#[tokio::test]
+async fn test_start_creates_unique_sessions() {
+    let (app, _state) = create_test_app().await;
+
+    // Create first session
+    let request1 = Request::builder()
+        .uri("/agent/start")
+        .method("POST")
+        .header("content-type", "application/json")
+        .header("x-secret-key", "test-secret")
+        .body(Body::from(
+            json!({
+                "working_dir": "/tmp/session1"
+            })
+            .to_string(),
+        ))
+        .unwrap();
+
+    let response1 = app.clone().oneshot(request1).await.unwrap();
+    assert_eq!(response1.status(), StatusCode::OK);
+
+    let body1 = body::to_bytes(response1.into_body(), usize::MAX)
+        .await
+        .unwrap();
+    let json1: Value = serde_json::from_slice(&body1).unwrap();
+    let session_id1 = json1["session_id"].as_str().unwrap();
+
+    // Create second session
+    let request2 = Request::builder()
+        .uri("/agent/start")
+        .method("POST")
+        .header("content-type", "application/json")
+        .header("x-secret-key", "test-secret")
+        .body(Body::from(
+            json!({
+                "working_dir": "/tmp/session2"
+            })
+            .to_string(),
+        ))
+        .unwrap();
+
+    let response2 = app.oneshot(request2).await.unwrap();
+    assert_eq!(response2.status(), StatusCode::OK);
+
+    let body2 = body::to_bytes(response2.into_body(), usize::MAX)
+        .await
+        .unwrap();
+    let json2: Value = serde_json::from_slice(&body2).unwrap();
+    let session_id2 = json2["session_id"].as_str().unwrap();
+
+    // Session IDs should be different
+    assert_ne!(session_id1, session_id2);
+
+    // Both should have metadata
+    assert_eq!(
+        json1["metadata"]["working_dir"].as_str().unwrap(),
+        "/tmp/session1"
+    );
+    assert_eq!(
+        json2["metadata"]["working_dir"].as_str().unwrap(),
+        "/tmp/session2"
+    );
+}
+
+#[tokio::test]
+async fn test_resume_retrieves_correct_session() {
+    let (app, _state) = create_test_app().await;
+
+    // Create a session first
+    let start_request = Request::builder()
+        .uri("/agent/start")
+        .method("POST")
+        .header("content-type", "application/json")
+        .header("x-secret-key", "test-secret")
+        .body(Body::from(
+            json!({
+                "working_dir": "/tmp/resume_test"
+            })
+            .to_string(),
+        ))
+        .unwrap();
+
+    let start_response = app.clone().oneshot(start_request).await.unwrap();
+    let start_body = body::to_bytes(start_response.into_body(), usize::MAX)
+        .await
+        .unwrap();
+    let start_json: Value = serde_json::from_slice(&start_body).unwrap();
+    let session_id = start_json["session_id"].as_str().unwrap();
+
+    // Resume the session
+    let resume_request = Request::builder()
+        .uri("/agent/resume")
+        .method("POST")
+        .header("content-type", "application/json")
+        .header("x-secret-key", "test-secret")
+        .body(Body::from(
+            json!({
+                "session_id": session_id
+            })
+            .to_string(),
+        ))
+        .unwrap();
+
+    let resume_response = app.oneshot(resume_request).await.unwrap();
+    assert_eq!(resume_response.status(), StatusCode::OK);
+
+    let resume_body = body::to_bytes(resume_response.into_body(), usize::MAX)
+        .await
+        .unwrap();
+    let resume_json: Value = serde_json::from_slice(&resume_body).unwrap();
+
+    // Should get back the same session
+    assert_eq!(resume_json["session_id"].as_str().unwrap(), session_id);
+    assert_eq!(
+        resume_json["metadata"]["working_dir"].as_str().unwrap(),
+        "/tmp/resume_test"
+    );
+}
+
+#[tokio::test]
+async fn test_tools_endpoint_with_session_isolation() {
+    let (app, state) = create_test_app().await;
+
+    // Create two sessions
+    let session1_id = "test_session_tools_1";
+    let session2_id = "test_session_tools_2";
+
+    // Get agents for both sessions to ensure they exist
+    let _ = state
+        .get_agent(goose::session::Identifier::Name(session1_id.to_string()))
+        .await
+        .unwrap();
+    let _ = state
+        .get_agent(goose::session::Identifier::Name(session2_id.to_string()))
+        .await
+        .unwrap();
+
+    // Get tools for session 1
+    let tools1_request = Request::builder()
+        .uri(&format!("/agent/tools?session_id={}", session1_id))
+        .method("GET")
+        .header("x-secret-key", "test-secret")
+        .body(Body::empty())
+        .unwrap();
+
+    let tools1_response = app.clone().oneshot(tools1_request).await.unwrap();
+    assert_eq!(tools1_response.status(), StatusCode::OK);
+
+    let tools1_body = body::to_bytes(tools1_response.into_body(), usize::MAX)
+        .await
+        .unwrap();
+    let tools1: Vec<Value> = serde_json::from_slice(&tools1_body).unwrap();
+
+    // Get tools for session 2
+    let tools2_request = Request::builder()
+        .uri(&format!("/agent/tools?session_id={}", session2_id))
+        .method("GET")
+        .header("x-secret-key", "test-secret")
+        .body(Body::empty())
+        .unwrap();
+
+    let tools2_response = app.oneshot(tools2_request).await.unwrap();
+    assert_eq!(tools2_response.status(), StatusCode::OK);
+
+    let tools2_body = body::to_bytes(tools2_response.into_body(), usize::MAX)
+        .await
+        .unwrap();
+    let tools2: Vec<Value> = serde_json::from_slice(&tools2_body).unwrap();
+
+    // Both should have base tools (at least platform tools)
+    assert!(!tools1.is_empty());
+    assert!(!tools2.is_empty());
+
+    // Should have similar base tools
+    let base_tool_names = [
+        "platform__manage_extensions",
+        "platform__search_available_extensions",
+    ];
+    for tool_name in &base_tool_names {
+        assert!(tools1.iter().any(|t| t["name"] == *tool_name));
+        assert!(tools2.iter().any(|t| t["name"] == *tool_name));
+    }
+}
+
+#[tokio::test]
+async fn test_update_provider_per_session() {
+    let (app, state) = create_test_app().await;
+
+    // Create two sessions
+    let session1_id = "provider_test_1";
+    let session2_id = "provider_test_2";
+
+    // Ensure agents exist
+    let _ = state
+        .get_agent(goose::session::Identifier::Name(session1_id.to_string()))
+        .await
+        .unwrap();
+    let _ = state
+        .get_agent(goose::session::Identifier::Name(session2_id.to_string()))
+        .await
+        .unwrap();
+
+    // Update provider for session 1 (this will fail but that's ok for the test)
+    let update1_request = Request::builder()
+        .uri("/agent/update_provider")
+        .method("POST")
+        .header("content-type", "application/json")
+        .header("x-secret-key", "test-secret")
+        .body(Body::from(
+            json!({
+                "session_id": session1_id,
+                "provider": "openai",
+                "model": "gpt-4"
+            })
+            .to_string(),
+        ))
+        .unwrap();
+
+    let update1_response = app.clone().oneshot(update1_request).await.unwrap();
+    // May fail due to missing API key, but the routing should work
+    assert!(
+        update1_response.status() == StatusCode::OK
+            || update1_response.status() == StatusCode::BAD_REQUEST
+    );
+
+    // Update provider for session 2
+    let update2_request = Request::builder()
+        .uri("/agent/update_provider")
+        .method("POST")
+        .header("content-type", "application/json")
+        .header("x-secret-key", "test-secret")
+        .body(Body::from(
+            json!({
+                "session_id": session2_id,
+                "provider": "anthropic",
+                "model": "claude-3-sonnet"
+            })
+            .to_string(),
+        ))
+        .unwrap();
+
+    let update2_response = app.oneshot(update2_request).await.unwrap();
+    assert!(
+        update2_response.status() == StatusCode::OK
+            || update2_response.status() == StatusCode::BAD_REQUEST
+    );
+}
diff --git a/crates/goose/src/agents/manager.rs b/crates/goose/src/agents/manager.rs
new file mode 100644
index 00000000000..4fcd30a037f
--- /dev/null
+++ b/crates/goose/src/agents/manager.rs
@@ -0,0 +1,403 @@
+use std::collections::HashMap;
+use std::sync::Arc;
+use std::time::Duration;
+
+use crate::agents::Agent;
+use crate::session;
+use chrono::{DateTime, Utc};
+use serde::{Deserialize, Serialize};
+use thiserror::Error;
+use tokio::sync::RwLock;
+
+/// Error types for AgentManager operations
+#[derive(Error, Debug)]
+pub enum AgentError {
+    #[error("Failed to create agent: {0}")]
+    CreationFailed(String),
+
+    #[error("Failed to acquire lock: {0}")]
+    LockError(String),
+
+    #[error("Agent not found for session: {0}")]
+    NotFound(String),
+
+    #[error("Configuration error: {0}")]
+    ConfigError(String),
+}
+
+/// Configuration for the AgentManager
+#[derive(Debug, Clone, Serialize, Deserialize)]
+pub struct AgentManagerConfig {
+    /// Maximum idle time before an agent is cleaned up
+    pub max_idle_duration: Duration,
+
+    /// Whether to enable agent pooling (future optimization)
+    pub enable_pooling: bool,
+
+    /// Maximum number of agents to keep in memory
+    pub max_agents: usize,
+
+    /// Interval for running cleanup
+    pub cleanup_interval: Duration,
+}
+
+impl Default for AgentManagerConfig {
+    fn default() -> Self {
+        Self {
+            max_idle_duration: Duration::from_secs(3600), // 1 hour
+            enable_pooling: false,
+            max_agents: 100,
+            cleanup_interval: Duration::from_secs(300), // 5 minutes
+        }
+    }
+}
+
+/// Execution mode for an agent
+#[derive(Debug, Clone, Serialize, Deserialize)]
+pub enum ExecutionMode {
+    /// Interactive mode - user is directly interacting
+    Interactive,
+
+    /// Background mode - running as scheduled job
+    Background,
+
+    /// SubTask mode - running as subtask of another agent
+    SubTask {
+        parent: session::Identifier,
+        inherit: InheritConfig,
+        approval_mode: ApprovalMode,
+    },
+}
+
+/// Configuration for what a subtask inherits from parent
+#[derive(Debug, Clone, Serialize, Deserialize)]
+pub struct InheritConfig {
+    pub extensions: bool,
+    pub provider: bool,
+    pub settings: bool,
+}
+
+impl Default for InheritConfig {
+    fn default() -> Self {
+        Self {
+            extensions: true,
+            provider: true,
+            settings: true,
+        }
+    }
+}
+
+/// Defines how subtask tool approvals are handled
+#[derive(Debug, Clone, Serialize, Deserialize)]
+pub enum ApprovalMode {
+    /// Subtask handles all approvals locally (default, backward compatible)
+    Autonomous,
+
+    /// All approval requests bubble to parent
+    BubbleAll,
+
+    /// Selective bubbling based on tool names
+    BubbleFiltered {
+        tools: Vec<String>,
+        default_action: ApprovalAction,
+    },
+}
+
+impl Default for ApprovalMode {
+    fn default() -> Self {
+        Self::Autonomous // Maintain backward compatibility
+    }
+}
+
+/// Default action for tools not in filter list
+#[derive(Debug, Clone, Serialize, Deserialize)]
+pub enum ApprovalAction {
+    Approve,
+    Deny,
+    Bubble,
+}
+
+/// State of a session's agent
+#[derive(Debug, Clone)]
+#[allow(dead_code)]
+enum SessionState {
+    Active,
+    Idle,
+    Executing,
+}
+
+/// Wrapper for an agent with session metadata
+struct SessionAgent {
+    agent: Arc<Agent>,
+    #[allow(dead_code)]
+    session_id: session::Identifier,
+    #[allow(dead_code)]
+    created_at: DateTime<Utc>,
+    last_used: DateTime<Utc>,
+    execution_mode: ExecutionMode,
+    #[allow(dead_code)]
+    state: SessionState,
+}
+
+/// Metrics for monitoring agent manager performance
+#[derive(Debug, Default)]
+pub struct AgentMetrics {
+    pub agents_created: usize,
+    pub agents_cleaned: usize,
+    pub cache_hits: usize,
+    pub cache_misses: usize,
+    pub active_agents: usize,
+}
+
+impl AgentMetrics {
+    fn record_agent_created(&mut self) {
+        self.agents_created += 1;
+        self.active_agents += 1;
+    }
+
+    fn record_cache_hit(&mut self) {
+        self.cache_hits += 1;
+    }
+
+    fn record_cache_miss(&mut self) {
+        self.cache_misses += 1;
+    }
+
+    fn record_cleanup(&mut self, count: usize) {
+        self.agents_cleaned += count;
+        self.active_agents = self.active_agents.saturating_sub(count);
+    }
+}
+
+/// Optional agent pooling for future optimization
+struct AgentPool {
+    // Future implementation for agent reuse
+    _placeholder: std::marker::PhantomData<()>,
+}
+
+/// Manages agent lifecycle and session mapping
+///
+/// This is the central component that ensures each session gets its own
+/// isolated agent instance, solving the shared agent concurrency issues.
+pub struct AgentManager {
+    /// Maps session IDs to their dedicated agents
+    agents: Arc<RwLock<HashMap<session::Identifier, SessionAgent>>>,
+
+    /// Optional pool for agent reuse (future optimization)
+    #[allow(dead_code)]
+    pool: Option<AgentPool>,
+
+    /// Configuration for agent creation and management
+    config: AgentManagerConfig,
+
+    /// Metrics for monitoring and debugging
+    metrics: Arc<RwLock<AgentMetrics>>,
+}
+
+impl AgentManager {
+    /// Create a new AgentManager with the given configuration
+    pub async fn new(config: AgentManagerConfig) -> Self {
+        Self {
+            agents: Arc::new(RwLock::new(HashMap::new())),
+            pool: None, // Start without pooling, add later
+            config,
+            metrics: Arc::new(RwLock::new(AgentMetrics::default())),
+        }
+    }
+
+    /// Get or create an agent for a session
+    ///
+    /// This ensures each session has exactly one agent, providing
+    /// complete isolation between users/sessions.
+    pub async fn get_agent(
+        &self,
+        session_id: session::Identifier,
+    ) -> Result<Arc<Agent>, AgentError> {
+        // First try to get existing agent with read lock
+        {
+            let agents = self.agents.read().await;
+            if let Some(session_agent) = agents.get(&session_id) {
+                // Update metrics
+                self.metrics.write().await.record_cache_hit();
+
+                // Clone Arc and return (last_used will be updated separately)
+                return Ok(Arc::clone(&session_agent.agent));
+            }
+        }
+
+        // Need to create new agent - acquire write lock
+        let mut agents = self.agents.write().await;
+
+        // Double-check in case another thread created it while we waited for write lock
+        if let Some(session_agent) = agents.get_mut(&session_id) {
+            session_agent.last_used = Utc::now();
+            self.metrics.write().await.record_cache_hit();
+            return Ok(Arc::clone(&session_agent.agent));
+        }
+
+        // Create new agent for this session
+        self.metrics.write().await.record_cache_miss();
+        let agent = self.create_agent_for_session(session_id.clone()).await?;
+
+        // Store the agent
+        agents.insert(
+            session_id.clone(),
+            SessionAgent {
+                agent: Arc::clone(&agent),
+                session_id: session_id.clone(),
+                created_at: Utc::now(),
+                last_used: Utc::now(),
+                execution_mode: ExecutionMode::Interactive,
+                state: SessionState::Active,
+            },
+        );
+
+        self.metrics.write().await.record_agent_created();
+        Ok(agent)
+    }
+
+    /// Update the last used time for a session's agent
+    pub async fn touch_session(&self, session_id: &session::Identifier) -> Result<(), AgentError> {
+        let mut agents = self.agents.write().await;
+        if let Some(session_agent) = agents.get_mut(session_id) {
+            session_agent.last_used = Utc::now();
+            Ok(())
+        } else {
+            Err(AgentError::NotFound(format!("{:?}", session_id)))
+        }
+    }
+
+    /// Get agent for a session with specific execution mode
+    pub async fn get_agent_with_mode(
+        &self,
+        session_id: session::Identifier,
+        mode: ExecutionMode,
+    ) -> Result<Arc<Agent>, AgentError> {
+        let agent = self.get_agent(session_id.clone()).await?;
+
+        // Update execution mode for the session
+        let mut agents = self.agents.write().await;
+        if let Some(session_agent) = agents.get_mut(&session_id) {
+            session_agent.execution_mode = mode;
+        }
+
+        Ok(agent)
+    }
+
+    /// Create a new agent for a session
+    async fn create_agent_for_session(
+        &self,
+        _session_id: session::Identifier,
+    ) -> Result<Arc<Agent>, AgentError> {
+        // For now, create a new agent directly
+        // In the future, this could use pooling or other optimizations
+
+        // Note: Agent::new() is synchronous, so we don't need await here
+        let agent = Agent::new();
+
+        // Initialize the agent with a provider from configuration
+        if let Err(e) = Self::initialize_agent_provider(&agent).await {
+            tracing::warn!("Failed to initialize provider for new agent: {}", e);
+            // Continue without provider - it can be set later via API
+        }
+
+        // TODO: Once Agent is updated with session support, use:
+        // let agent = Agent::new_with_session(session_id, ExecutionMode::Interactive);
+
+        Ok(Arc::new(agent))
+    }
+
+    /// Initialize an agent with a provider from configuration
+    async fn initialize_agent_provider(agent: &Agent) -> Result<(), AgentError> {
+        use crate::config::Config;
+        use crate::model::ModelConfig;
+        use crate::providers::create;
+
+        let config = Config::global();
+
+        // Get provider and model from environment/config
+        let provider_name = config
+            .get_param::<String>("GOOSE_PROVIDER")
+            .map_err(|e| AgentError::ConfigError(format!("No provider configured: {}", e)))?;
+
+        let model_name = config
+            .get_param::<String>("GOOSE_MODEL")
+            .map_err(|e| AgentError::ConfigError(format!("No model configured: {}", e)))?;
+
+        // Create model configuration
+        let model_config = ModelConfig::new(&model_name)
+            .map_err(|e| AgentError::ConfigError(format!("Invalid model config: {}", e)))?;
+
+        // Create provider
+        let provider = create(&provider_name, model_config)
+            .map_err(|e| AgentError::CreationFailed(format!("Failed to create provider: {}", e)))?;
+
+        // Set the provider on the agent
+        agent
+            .update_provider(provider)
+            .await
+            .map_err(|e| AgentError::CreationFailed(format!("Failed to set provider: {}", e)))?;
+
+        Ok(())
+    }
+
+    /// Clean up idle agents to manage memory
+    ///
+    /// Following the pattern from session::storage::cleanup_old_sessions
+    pub async fn cleanup_idle(&self, max_idle: Duration) -> Result<usize, AgentError> {
+        let mut agents = self.agents.write().await;
+        let now = Utc::now();
+        let mut removed = 0;
+
+        agents.retain(|_, session_agent| {
+            let idle_time = now.signed_duration_since(session_agent.last_used);
+            if idle_time > chrono::Duration::from_std(max_idle).unwrap() {
+                removed += 1;
+                false
+            } else {
+                true
+            }
+        });
+
+        self.metrics.write().await.record_cleanup(removed);
+        Ok(removed)
+    }
+
+    /// Run cleanup based on configured interval
+    pub async fn cleanup(&self) -> Result<usize, AgentError> {
+        self.cleanup_idle(self.config.max_idle_duration).await
+    }
+
+    /// Get current metrics
+    pub async fn get_metrics(&self) -> AgentMetrics {
+        let metrics = self.metrics.read().await;
+        AgentMetrics {
+            agents_created: metrics.agents_created,
+            agents_cleaned: metrics.agents_cleaned,
+            cache_hits: metrics.cache_hits,
+            cache_misses: metrics.cache_misses,
+            active_agents: metrics.active_agents,
+        }
+    }
+
+    /// Get number of active agents
+    pub async fn active_agent_count(&self) -> usize {
+        self.agents.read().await.len()
+    }
+
+    /// Remove a specific session's agent
+    pub async fn remove_agent(&self, session_id: &session::Identifier) -> Result<(), AgentError> {
+        let mut agents = self.agents.write().await;
+        if agents.remove(session_id).is_some() {
+            self.metrics.write().await.record_cleanup(1);
+            Ok(())
+        } else {
+            Err(AgentError::NotFound(format!("{:?}", session_id)))
+        }
+    }
+
+    /// Check if a session has an active agent
+    pub async fn has_agent(&self, session_id: &session::Identifier) -> bool {
+        self.agents.read().await.contains_key(session_id)
+    }
+}
diff --git a/crates/goose/src/agents/mod.rs b/crates/goose/src/agents/mod.rs
index 7782d22d98a..b2af15fd7a9 100644
--- a/crates/goose/src/agents/mod.rs
+++ b/crates/goose/src/agents/mod.rs
@@ -5,6 +5,7 @@ pub mod extension_malware_check;
 pub mod extension_manager;
 pub mod final_output_tool;
 mod large_response_handler;
+pub mod manager;
 pub mod model_selector;
 pub mod platform_tools;
 pub mod prompt_manager;
diff --git a/crates/goose/src/session/storage.rs b/crates/goose/src/session/storage.rs
index 66f76146919..e6221d080a1 100644
--- a/crates/goose/src/session/storage.rs
+++ b/crates/goose/src/session/storage.rs
@@ -157,7 +157,7 @@ impl Default for SessionMetadata {
 // The single app name used for all Goose applications
 const APP_NAME: &str = "goose";
 
-#[derive(Debug, Clone, Serialize, Deserialize)]
+#[derive(Debug, Clone, Serialize, Deserialize, PartialEq, Eq, Hash)]
 pub enum Identifier {
     Name(String),
     Path(PathBuf),
diff --git a/crates/goose/tests/agent_manager_extension_test.rs b/crates/goose/tests/agent_manager_extension_test.rs
new file mode 100644
index 00000000000..4f85f170a87
--- /dev/null
+++ b/crates/goose/tests/agent_manager_extension_test.rs
@@ -0,0 +1,239 @@
+use std::sync::Arc;
+
+use goose::agents::extension::ExtensionConfig;
+use goose::agents::manager::{AgentManager, AgentManagerConfig};
+use goose::session;
+use rmcp::model::Tool;
+
+/// Create a simple frontend extension for testing
+fn create_test_extension(name: &str) -> ExtensionConfig {
+    ExtensionConfig::Frontend {
+        name: name.to_string(),
+        tools: vec![Tool {
+            name: format!("{}_tool", name).into(),
+            description: Some(format!("Tool from {} extension", name).into()),
+            input_schema: Arc::new(
+                serde_json::json!({
+                    "type": "object",
+                    "properties": {}
+                })
+                .as_object()
+                .unwrap()
+                .clone(),
+            ),
+            output_schema: None,
+            annotations: None,
+        }],
+        instructions: Some(format!("Instructions for {}", name)),
+        bundled: Some(false),
+        available_tools: vec![],
+    }
+}
+
+#[tokio::test]
+async fn test_extension_isolation_between_sessions() {
+    // Extensions added to one session should not appear in another
+    let manager = AgentManager::new(AgentManagerConfig::default()).await;
+
+    let session1 = session::Identifier::Name("ext_test_1".to_string());
+    let session2 = session::Identifier::Name("ext_test_2".to_string());
+
+    let agent1 = manager.get_agent(session1.clone()).await.unwrap();
+    let agent2 = manager.get_agent(session2.clone()).await.unwrap();
+
+    // Add different extensions to each agent
+    let ext1 = create_test_extension("extension1");
+    let ext2 = create_test_extension("extension2");
+
+    agent1.add_extension(ext1).await.unwrap();
+    agent2.add_extension(ext2).await.unwrap();
+
+    // Check tools for each agent
+    let tools1 = agent1.list_tools(None).await;
+    let tools2 = agent2.list_tools(None).await;
+
+    // Agent1 should have extension1_tool but not extension2_tool
+    assert!(tools1.iter().any(|t| t.name == "extension1_tool"));
+    assert!(!tools1.iter().any(|t| t.name == "extension2_tool"));
+
+    // Agent2 should have extension2_tool but not extension1_tool
+    assert!(tools2.iter().any(|t| t.name == "extension2_tool"));
+    assert!(!tools2.iter().any(|t| t.name == "extension1_tool"));
+}
+
+#[tokio::test]
+async fn test_extension_persistence_within_session() {
+    // Extensions should persist when an agent is retrieved multiple times
+    let manager = AgentManager::new(AgentManagerConfig::default()).await;
+    let session = session::Identifier::Name("ext_persistence".to_string());
+
+    // Add extension to agent
+    let agent1 = manager.get_agent(session.clone()).await.unwrap();
+    let ext = create_test_extension("persistent");
+    agent1.add_extension(ext).await.unwrap();
+
+    // Verify extension is present
+    let tools1 = agent1.list_tools(None).await;
+    assert!(tools1.iter().any(|t| t.name == "persistent_tool"));
+
+    // Get agent again (from cache)
+    let agent2 = manager.get_agent(session.clone()).await.unwrap();
+
+    // Extension should still be present
+    let tools2 = agent2.list_tools(None).await;
+    assert!(tools2.iter().any(|t| t.name == "persistent_tool"));
+
+    // Verify it's the same agent instance
+    assert!(Arc::ptr_eq(&agent1, &agent2));
+}
+
+#[tokio::test]
+async fn test_extension_removal_isolation() {
+    // Removing an extension from one session shouldn't affect others
+    let manager = AgentManager::new(AgentManagerConfig::default()).await;
+
+    let session1 = session::Identifier::Name("ext_remove_1".to_string());
+    let session2 = session::Identifier::Name("ext_remove_2".to_string());
+
+    let agent1 = manager.get_agent(session1).await.unwrap();
+    let agent2 = manager.get_agent(session2).await.unwrap();
+
+    // Add same extension to both agents
+    let ext1 = create_test_extension("shared");
+    let ext2 = create_test_extension("shared");
+
+    agent1.add_extension(ext1).await.unwrap();
+    agent2.add_extension(ext2).await.unwrap();
+
+    // Verify both have the extension
+    assert!(agent1
+        .list_tools(None)
+        .await
+        .iter()
+        .any(|t| t.name == "shared_tool"));
+    assert!(agent2
+        .list_tools(None)
+        .await
+        .iter()
+        .any(|t| t.name == "shared_tool"));
+
+    // Remove from agent1 only
+    agent1.remove_extension("shared").await.unwrap();
+
+    // Agent1 should not have it anymore
+    assert!(!agent1
+        .list_tools(None)
+        .await
+        .iter()
+        .any(|t| t.name == "shared_tool"));
+
+    // Agent2 should still have it
+    assert!(agent2
+        .list_tools(None)
+        .await
+        .iter()
+        .any(|t| t.name == "shared_tool"));
+}
+
+#[tokio::test]
+async fn test_concurrent_extension_operations() {
+    // Test that concurrent extension operations on different sessions don't interfere
+    let manager = Arc::new(AgentManager::new(AgentManagerConfig::default()).await);
+
+    let mut handles = vec![];
+
+    for i in 0..10 {
+        let manager_clone = Arc::clone(&manager);
+
+        let handle = tokio::spawn(async move {
+            let session = session::Identifier::Name(format!("concurrent_ext_{}", i));
+            let agent = manager_clone.get_agent(session).await.unwrap();
+
+            // Add multiple extensions
+            for j in 0..3 {
+                let ext = create_test_extension(&format!("ext_{}_{}", i, j));
+                agent.add_extension(ext).await.unwrap();
+            }
+
+            // Verify all extensions are present
+            let tools = agent.list_tools(None).await;
+            for j in 0..3 {
+                let tool_name = format!("ext_{}_{}__tool", i, j);
+                assert!(
+                    tools.iter().any(|t| t.name == tool_name),
+                    "Missing tool: {}",
+                    tool_name
+                );
+            }
+
+            // Remove one extension
+            agent
+                .remove_extension(&format!("ext_{}_1", i))
+                .await
+                .unwrap();
+
+            // Verify it's gone but others remain
+            let tools_after = agent.list_tools(None).await;
+            assert!(!tools_after
+                .iter()
+                .any(|t| t.name == format!("ext_{}_1__tool", i)));
+            assert!(tools_after
+                .iter()
+                .any(|t| t.name == format!("ext_{}_0__tool", i)));
+            assert!(tools_after
+                .iter()
+                .any(|t| t.name == format!("ext_{}_2__tool", i)));
+        });
+
+        handles.push(handle);
+    }
+
+    // Wait for all tasks to complete
+    for handle in handles {
+        handle.await.unwrap();
+    }
+}
+
+#[tokio::test]
+async fn test_extension_state_after_cleanup() {
+    // Test that extensions are lost after agent cleanup and recreation
+    let config = AgentManagerConfig {
+        max_idle_duration: std::time::Duration::from_millis(1), // Very short for testing
+        ..Default::default()
+    };
+
+    let manager = AgentManager::new(config).await;
+    let session = session::Identifier::Name("cleanup_ext_test".to_string());
+
+    // Add extension to agent
+    let agent1 = manager.get_agent(session.clone()).await.unwrap();
+    let ext = create_test_extension("temporary");
+    agent1.add_extension(ext.clone()).await.unwrap();
+
+    // Verify extension is present
+    assert!(agent1
+        .list_tools(None)
+        .await
+        .iter()
+        .any(|t| t.name == "temporary_tool"));
+
+    // Wait for idle timeout
+    tokio::time::sleep(std::time::Duration::from_millis(10)).await;
+
+    // Trigger cleanup
+    let removed = manager.cleanup().await.unwrap();
+    assert_eq!(removed, 1, "Should have cleaned up one agent");
+
+    // Get agent again (should be new instance)
+    let agent2 = manager.get_agent(session.clone()).await.unwrap();
+
+    // Extension should be gone (fresh agent)
+    assert!(!agent2
+        .list_tools(None)
+        .await
+        .iter()
+        .any(|t| t.name == "temporary_tool"));
+
+    // Verify it's a different agent instance
+    assert!(!Arc::ptr_eq(&agent1, &agent2));
+}
diff --git a/crates/goose/tests/agent_manager_provider_test.rs b/crates/goose/tests/agent_manager_provider_test.rs
new file mode 100644
index 00000000000..2368d481dd9
--- /dev/null
+++ b/crates/goose/tests/agent_manager_provider_test.rs
@@ -0,0 +1,208 @@
+use std::sync::atomic::{AtomicUsize, Ordering};
+use std::sync::Arc;
+use std::time::Duration;
+
+use goose::agents::manager::{AgentManager, AgentManagerConfig};
+use goose::conversation::message::Message;
+use goose::conversation::Conversation;
+use goose::model::ModelConfig;
+use goose::providers::base::{Provider, ProviderMetadata, ProviderUsage, Usage};
+use goose::providers::errors::ProviderError;
+use goose::session;
+use rmcp::model::Tool;
+
+/// Mock provider that tracks which instance is being used
+struct TrackingMockProvider {
+    id: usize,
+    model_config: ModelConfig,
+    fail_on_complete: bool,
+}
+
+impl TrackingMockProvider {
+    fn new(id: usize, fail: bool) -> Self {
+        Self {
+            id,
+            model_config: ModelConfig::new_or_fail("mock-model"),
+            fail_on_complete: fail,
+        }
+    }
+}
+
+#[async_trait::async_trait]
+impl Provider for TrackingMockProvider {
+    fn metadata() -> ProviderMetadata {
+        ProviderMetadata::empty()
+    }
+
+    async fn complete_with_model(
+        &self,
+        _model_config: &ModelConfig,
+        _system: &str,
+        _messages: &[Message],
+        _tools: &[Tool],
+    ) -> Result<(Message, ProviderUsage), ProviderError> {
+        if self.fail_on_complete {
+            return Err(ProviderError::ServerError("Mock failure".to_string()));
+        }
+
+        Ok((
+            Message::assistant().with_text(format!("Response from provider {}", self.id)),
+            ProviderUsage::new("mock".to_string(), Usage::default()),
+        ))
+    }
+
+    fn get_model_config(&self) -> ModelConfig {
+        self.model_config.clone()
+    }
+}
+
+#[tokio::test]
+async fn test_provider_isolation_between_sessions() {
+    // Each session should be able to have its own provider configuration
+    let manager = AgentManager::new(AgentManagerConfig::default()).await;
+
+    let session1 = session::Identifier::Name("provider_test_1".to_string());
+    let session2 = session::Identifier::Name("provider_test_2".to_string());
+
+    // Get agents for both sessions
+    let agent1 = manager.get_agent(session1.clone()).await.unwrap();
+    let agent2 = manager.get_agent(session2.clone()).await.unwrap();
+
+    // Set different providers for each agent
+    let provider1 = Arc::new(TrackingMockProvider::new(1, false));
+    let provider2 = Arc::new(TrackingMockProvider::new(2, false));
+
+    agent1.update_provider(provider1).await.unwrap();
+    agent2.update_provider(provider2).await.unwrap();
+
+    // Create test conversations
+    let conv = Conversation::new(vec![Message::user().with_text("test")]).unwrap();
+
+    // Complete with each agent and verify they use different providers
+    let (response1, _) = agent1
+        .provider()
+        .await
+        .unwrap()
+        .complete("system", conv.messages(), &[])
+        .await
+        .unwrap();
+
+    let (response2, _) = agent2
+        .provider()
+        .await
+        .unwrap()
+        .complete("system", conv.messages(), &[])
+        .await
+        .unwrap();
+
+    // Verify responses come from different providers
+    assert!(response1.as_concat_text().contains("provider 1"));
+    assert!(response2.as_concat_text().contains("provider 2"));
+}
+
+#[tokio::test]
+async fn test_provider_initialization_failure_handling() {
+    // Test that agent creation succeeds even if provider initialization fails
+    // This is important because providers are initialized from environment variables
+    // which might not be set correctly
+
+    // Temporarily set invalid provider config
+    std::env::set_var("GOOSE_PROVIDER", "invalid_provider");
+    std::env::set_var("GOOSE_MODEL", "invalid_model");
+
+    let manager = AgentManager::new(AgentManagerConfig::default()).await;
+    let session = session::Identifier::Name("provider_fail_test".to_string());
+
+    // This should succeed even though provider initialization will fail
+    let agent = manager.get_agent(session.clone()).await;
+    assert!(
+        agent.is_ok(),
+        "Agent creation should succeed even with invalid provider config"
+    );
+
+    // Clean up
+    std::env::remove_var("GOOSE_PROVIDER");
+    std::env::remove_var("GOOSE_MODEL");
+}
+
+#[tokio::test]
+async fn test_concurrent_provider_updates() {
+    // Test that concurrent provider updates to different sessions don't interfere
+    let manager = Arc::new(AgentManager::new(AgentManagerConfig::default()).await);
+    let update_counter = Arc::new(AtomicUsize::new(0));
+
+    let mut handles = vec![];
+
+    for i in 0..10 {
+        let manager_clone = Arc::clone(&manager);
+        let counter_clone = Arc::clone(&update_counter);
+
+        let handle = tokio::spawn(async move {
+            let session = session::Identifier::Name(format!("concurrent_provider_{}", i));
+            let agent = manager_clone.get_agent(session).await.unwrap();
+
+            // Each task updates its agent's provider multiple times
+            for j in 0..5 {
+                let provider = Arc::new(TrackingMockProvider::new(i * 100 + j, false));
+                agent.update_provider(provider).await.unwrap();
+                counter_clone.fetch_add(1, Ordering::SeqCst);
+                tokio::time::sleep(Duration::from_millis(1)).await;
+            }
+
+            // Verify final provider is correct
+            let conv = Conversation::new(vec![Message::user().with_text("test")]).unwrap();
+            let (response, _) = agent
+                .provider()
+                .await
+                .unwrap()
+                .complete("system", conv.messages(), &[])
+                .await
+                .unwrap();
+
+            // Should have the last provider for this session
+            assert!(response
+                .as_concat_text()
+                .contains(&format!("provider {}", i * 100 + 4)));
+        });
+
+        handles.push(handle);
+    }
+
+    // Wait for all tasks to complete
+    for handle in handles {
+        handle.await.unwrap();
+    }
+
+    // Verify all updates completed
+    assert_eq!(update_counter.load(Ordering::SeqCst), 50);
+}
+
+#[tokio::test]
+async fn test_provider_persistence_across_agent_retrievals() {
+    // Test that a provider set on an agent persists when the agent is retrieved again
+    let manager = AgentManager::new(AgentManagerConfig::default()).await;
+    let session = session::Identifier::Name("provider_persistence".to_string());
+
+    // Get agent and set provider
+    let agent1 = manager.get_agent(session.clone()).await.unwrap();
+    let provider = Arc::new(TrackingMockProvider::new(42, false));
+    agent1.update_provider(provider).await.unwrap();
+
+    // Get the same agent again (should be cached)
+    let agent2 = manager.get_agent(session.clone()).await.unwrap();
+
+    // Verify it has the same provider
+    let conv = Conversation::new(vec![Message::user().with_text("test")]).unwrap();
+    let (response, _) = agent2
+        .provider()
+        .await
+        .unwrap()
+        .complete("system", conv.messages(), &[])
+        .await
+        .unwrap();
+
+    assert!(response.as_concat_text().contains("provider 42"));
+
+    // Verify they're the same agent instance
+    assert!(Arc::ptr_eq(&agent1, &agent2));
+}
diff --git a/crates/goose/tests/agent_manager_test.rs b/crates/goose/tests/agent_manager_test.rs
new file mode 100644
index 00000000000..f54ceb78d43
--- /dev/null
+++ b/crates/goose/tests/agent_manager_test.rs
@@ -0,0 +1,192 @@
+use goose::agents::manager::AgentManager;
+use goose::session;
+use std::sync::Arc;
+use std::time::Duration;
+
+#[tokio::test]
+async fn test_agent_per_session() {
+    // Verify each session gets unique agent
+    let manager = AgentManager::new(Default::default()).await;
+    let session1 = session::Identifier::Name("session1".to_string());
+    let session2 = session::Identifier::Name("session2".to_string());
+
+    let agent1 = manager.get_agent(session1.clone()).await.unwrap();
+    let agent2 = manager.get_agent(session2.clone()).await.unwrap();
+
+    // Different sessions get different agents
+    assert!(!Arc::ptr_eq(&agent1, &agent2));
+
+    // Same session gets same agent
+    let agent1_again = manager.get_agent(session1).await.unwrap();
+    assert!(Arc::ptr_eq(&agent1, &agent1_again));
+}
+
+#[tokio::test]
+async fn test_cleanup_idle_agents() {
+    // Verify idle agents are cleaned up
+    let manager = AgentManager::new(Default::default()).await;
+    let session = session::Identifier::Name("cleanup_test".to_string());
+
+    let _agent = manager.get_agent(session.clone()).await.unwrap();
+
+    // Verify agent exists
+    assert!(manager.has_agent(&session).await);
+
+    // Immediately cleanup with 0 idle time
+    let removed = manager.cleanup_idle(Duration::from_secs(0)).await.unwrap();
+    assert_eq!(removed, 1);
+
+    // Verify agent was removed
+    assert!(!manager.has_agent(&session).await);
+
+    // Agent should be recreated on next access
+    let _agent_new = manager.get_agent(session.clone()).await.unwrap();
+    assert!(manager.has_agent(&session).await);
+}
+
+#[tokio::test]
+async fn test_metrics_tracking() {
+    let manager = AgentManager::new(Default::default()).await;
+
+    // Create some agents
+    let session1 = session::Identifier::Name("metrics1".to_string());
+    let session2 = session::Identifier::Name("metrics2".to_string());
+
+    let _agent1 = manager.get_agent(session1.clone()).await.unwrap();
+    let _agent2 = manager.get_agent(session2.clone()).await.unwrap();
+
+    // Access same session again (cache hit)
+    let _agent1_again = manager.get_agent(session1).await.unwrap();
+
+    let metrics = manager.get_metrics().await;
+    assert_eq!(metrics.agents_created, 2);
+    assert_eq!(metrics.cache_hits, 1);
+    assert_eq!(metrics.cache_misses, 2);
+    assert_eq!(metrics.active_agents, 2);
+
+    // Cleanup
+    let removed = manager.cleanup_idle(Duration::from_secs(0)).await.unwrap();
+    assert_eq!(removed, 2);
+
+    let metrics = manager.get_metrics().await;
+    assert_eq!(metrics.agents_cleaned, 2);
+    assert_eq!(metrics.active_agents, 0);
+}
+
+#[tokio::test]
+async fn test_concurrent_access() {
+    use tokio::task;
+
+    let manager = Arc::new(AgentManager::new(Default::default()).await);
+    let session = session::Identifier::Name("concurrent".to_string());
+
+    // Spawn multiple tasks accessing the same session
+    let mut handles = vec![];
+    for _ in 0..10 {
+        let manager_clone = Arc::clone(&manager);
+        let session_clone = session.clone();
+        handles.push(task::spawn(async move {
+            manager_clone.get_agent(session_clone).await.unwrap()
+        }));
+    }
+
+    // Collect all agents
+    let mut agents = vec![];
+    for handle in handles {
+        agents.push(handle.await.unwrap());
+    }
+
+    // All should be the same agent
+    for agent in &agents[1..] {
+        assert!(Arc::ptr_eq(&agents[0], agent));
+    }
+
+    // Only one agent should have been created
+    let metrics = manager.get_metrics().await;
+    assert_eq!(metrics.agents_created, 1);
+    assert_eq!(metrics.cache_hits, 9);
+}
+
+#[tokio::test]
+async fn test_remove_specific_agent() {
+    let manager = AgentManager::new(Default::default()).await;
+    let session = session::Identifier::Name("remove_test".to_string());
+
+    // Create agent
+    let _agent = manager.get_agent(session.clone()).await.unwrap();
+    assert!(manager.has_agent(&session).await);
+
+    // Remove specific agent
+    manager.remove_agent(&session).await.unwrap();
+    assert!(!manager.has_agent(&session).await);
+
+    // Removing again should error
+    let result = manager.remove_agent(&session).await;
+    assert!(result.is_err());
+}
+
+#[tokio::test]
+async fn test_execution_mode_update() {
+    use goose::agents::manager::{ApprovalMode, ExecutionMode, InheritConfig};
+
+    let manager = AgentManager::new(Default::default()).await;
+    let session = session::Identifier::Name("exec_mode".to_string());
+    let parent_session = session::Identifier::Name("parent".to_string());
+
+    // Get agent with specific mode
+    let mode = ExecutionMode::SubTask {
+        parent: parent_session,
+        inherit: InheritConfig::default(),
+        approval_mode: ApprovalMode::default(),
+    };
+
+    let _agent = manager
+        .get_agent_with_mode(session.clone(), mode.clone())
+        .await
+        .unwrap();
+
+    // Verify the mode was set (would need access to internal state in real implementation)
+    assert!(manager.has_agent(&session).await);
+}
+
+#[tokio::test]
+async fn test_touch_session() {
+    let manager = AgentManager::new(Default::default()).await;
+    let session = session::Identifier::Name("touch_test".to_string());
+
+    // Create agent
+    let _agent = manager.get_agent(session.clone()).await.unwrap();
+
+    // Touch should succeed
+    manager.touch_session(&session).await.unwrap();
+
+    // Touch non-existent session should fail
+    let non_existent = session::Identifier::Name("nonexistent".to_string());
+    let result = manager.touch_session(&non_existent).await;
+    assert!(result.is_err());
+}
+
+#[tokio::test]
+async fn test_active_agent_count() {
+    let manager = AgentManager::new(Default::default()).await;
+
+    assert_eq!(manager.active_agent_count().await, 0);
+
+    // Create some agents
+    let session1 = session::Identifier::Name("count1".to_string());
+    let session2 = session::Identifier::Name("count2".to_string());
+    let session3 = session::Identifier::Name("count3".to_string());
+
+    let _agent1 = manager.get_agent(session1.clone()).await.unwrap();
+    assert_eq!(manager.active_agent_count().await, 1);
+
+    let _agent2 = manager.get_agent(session2).await.unwrap();
+    assert_eq!(manager.active_agent_count().await, 2);
+
+    let _agent3 = manager.get_agent(session3).await.unwrap();
+    assert_eq!(manager.active_agent_count().await, 3);
+
+    // Remove one
+    manager.remove_agent(&session1).await.unwrap();
+    assert_eq!(manager.active_agent_count().await, 2);
+}
diff --git a/ui/desktop/openapi.json b/ui/desktop/openapi.json
index 32875900c01..33024efef05 100644
--- a/ui/desktop/openapi.json
+++ b/ui/desktop/openapi.json
@@ -1702,7 +1702,8 @@
         "description": "Request payload for context management operations",
         "required": [
           "messages",
-          "manageAction"
+          "manageAction",
+          "sessionId"
         ],
         "properties": {
           "manageAction": {
@@ -1715,6 +1716,10 @@
               "$ref": "#/components/schemas/Message"
             },
             "description": "Collection of messages to be managed"
+          },
+          "sessionId": {
+            "type": "string",
+            "description": "Session ID for the context management"
           }
         }
       },
@@ -1782,7 +1787,8 @@
         "required": [
           "messages",
           "title",
-          "description"
+          "description",
+          "session_id"
         ],
         "properties": {
           "activities": {
@@ -1809,6 +1815,9 @@
               "$ref": "#/components/schemas/Message"
             }
           },
+          "session_id": {
+            "type": "string"
+          },
           "title": {
             "type": "string"
           }
